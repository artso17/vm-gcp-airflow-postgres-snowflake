# ETL Data Pipeline Using Apache Airflow-Postgres-Google Cloud Platform-Snowflake Cloud

## Project Brief
The project aim to build ETL Data pipeline with Airflow as Orchestrator or Scheduler service, Postgres as Data Source, and Snowflake Cloud as Data Warehouse. The Project build on Virtual Machine (vm) provided by Google Cloud Platform.
![Blank diagram](https://github.com/artso17/vm-gcp-airflow-postgres-snowflake/assets/78079780/bea0e9e4-4842-4263-990d-0f0b7eca11b6)

## Success Criteria
- Create Extract and Load to Snowflake Data Warehouse script with format name `[name]_etl1.py`
- Create Data Mart `Daily Gross Revenue` script with format name `[name]_etl2.py`
- Create Airflow scheduler using Bash Operator to run script with format name `[name]_dag.py`
- Run on VM  

## Result
- Create Extract and Load to Snowflake Data Warehouse script with name `aditya_etl1.py`
- Create Data Mart `Daily Gross Revenue` script with name `aditya_etl2.py`
- Create Airflow scheduler using Bash Operator to run script with name `aditya_dag.py`
- Running on Mentor's VM Google Cloud Platform - *The Server maybe down*
- Grade 100/100 from Mentor
